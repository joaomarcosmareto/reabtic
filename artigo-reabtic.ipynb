{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDENTIFICAÇÃO DO PROJETO\n",
    "**Autor:** João Marcos Mareto Calado\n",
    "<br />**Linguagem:** Python 3 em sua última versão no ambiente do Anaconda todo atualizado\n",
    "<br />**Dependências:** Este projeto depende das bibliotecas sklearn, nltk, numpy e difflib\n",
    "<br />**Título:** Avaliação de modelos de classificação na resolução do problema de associação de perfis de usuáros de serviços diferentes.\n",
    "\n",
    "### DESCRIÇÃO RESUMIDA DO PROJETO\n",
    "\n",
    "Foi solicitado como trabalho final da disciplina de Ciência de Dados, que o aluno fizesse um trabalho utilizando as técnicas aprendidas em sala de aula para realizar a análise exploratória de alguma base de dados e algum tipo de classificação ou regressão conforme necessidade.\n",
    "\n",
    "Este trabalho pretende aproveitar o que foi entregue como trabalho final da disciplina de Inteligência Artificial, porém com um enfoque maior na análise exploratória de dados, avaliando as características do dataset. Ao final, alguns algoritmos de classificação são utilizados para que o obtiver a melhor performance no critério de acurácia seja testado.\n",
    "\n",
    "### DESCRIÇÃO DO PROBLEMA\n",
    "\n",
    "Atualmente as pessoas passam um tempo considerável nas mais diversas redes sociais, criando um ambiente virtual onde elas se conectam à amigos, compartilham informações e expandem os laços sociais. Ter a capacidade de ligar os perfis nas diversas redes sociais poderia levar a um maior entendimento sobre o comportamento e costume dos usuários, permitindo a melhora na provisão e customização de serviços, além de recomendações melhores.\n",
    "\n",
    "Conforme definido porCarmagnola e Cena em 2009, este tipo de problema é definido como \"Cross-system Personalisation\" e a utilização de técnicas de ciência de dados tem obtido resultados bastante promissores.\n",
    "\n",
    "De acordo com Esfandyari et al. (2018), este é um problema difícil de ser resolvido dada a não estruturação das informações além da falta de garantia na veracidade das informações preenchidas. Corroborando com Esfandyari e colegas, Shu et al. (2017) complementa as dificuldades e desafios da tarefa de identificar os perfis em diferentes redes sociais. São elencados dois motivos pelos quais existe essa dificuldade. O primeiro é que embora usuários tenham contas em diferentes redes, a informação de uma mesma pessoa no mundo real pode ser diversa entre as redes, e o segundo motivo é que as informações da identidade dos usuários é ruidosa, incompleta e altamente não estruturada.\n",
    "\n",
    "O problema de identificação de usuário em sistemas cruzados, é definido da seguinte forma: dados dois perfis $P^{s1}$ e $P^{s2}$ de duas redes sociais diferentes $s1$ e $s2$, determinar se estes perfis pertencem à mesma pessoa. Isto corresponde à aprender uma função de identificação $f(P^{s1}, P^{s2})$, tal que:\n",
    "\n",
    "\\begin{equation}\n",
    "f(P^{s1}, P^{s2}) = \\left\\{ \\begin{array}{cl}\n",
    "1 & \\textrm{se } P^{s1} \\textrm{ e } P^{s2} \\textrm{, pertencem à mesma pessoa}\\\\\n",
    "0 & \\textrm{, caso contrário}\\\\\n",
    "\\end{array}\\right.\n",
    "\\end{equation}\n",
    "\n",
    "Para tentar resolver esses desafios, diversas pesquisas vem sendo realizadas utilizando abordagens diferentes. As abordagens, porém, podem ser agrupadas em (i) identificação baseada em nome de usuários, (ii) identificação baseada em perfis de usuários e a (iii) identificação baseada em conteúdo e rede de amigos.\n",
    "\n",
    "### RELATO DE ATIVIDADES\n",
    "\n",
    "O relato de atividades é composto por este Jupyter notebook. No decorrer deste documento, terá seções compostas de células de texto contendo uma descrição do passo pretendido e de células contendo o código-fonte correspondente.\n",
    "\n",
    "#### Base de Dados\n",
    "\n",
    "Para o trabalho, foi utilizada uma base de dados pública, disponível no portal do Laboratório de Protocolo de Redes e Tecnologias (NPTLab) da Universidade de Milão, criada por Esfandyari e colegas em 2018.\n",
    "\n",
    "A base de dados, denominada \"GT dataset\", contém 10.571 pares de perfis corretamente rotulados de usuários do <em>Google+</em> e <em>Twitter</em>.\n",
    "\n",
    "Esta base de dados disponibiliza 15 atributos em cada registro, sendo 6 do perfil do <em>Google+</em>, 8 do perfil do <em>Twitter</em> e um atributo de identificação do registro da base de dados. Os atributos são listados a seguir:\n",
    "\n",
    "- _id: atributo identificador do registro na base de dados;\n",
    "- Gid: atributo identificador do perfil no <em>Google+</em>;\n",
    "- G_Firstname: atributo que representa o primeiro nome do usuário no <em>Google+</em>;\n",
    "- G_Lastname: atributo que representa o último nome do usuário no <em>Google+</em>;\n",
    "- G_Displayname: atributo que representa o nome de usuário no <em>Google+</em>;\n",
    "- G_Location: atributo que representa a localização do usuário no <em>Google+</em>;\n",
    "- G_aboutme: atributo que contém uma descrição a respeito do usuário no <em>Google+</em>;\n",
    "- Tid: atributo identificador do usuário no <em>Twitter</em>;\n",
    "- T_Fullname: atributo que representa o nome completo do usuário no <em>Twitter</em>;\n",
    "- T_ScreenName: atributo que representa o nome de usuário no <em>Twitter</em>;\n",
    "- T_Location: atributo que representa a localização do usuário no <em>Twitter</em>;\n",
    "- T_Description: atributo que contém uma descrição a respeito do usuário no <em>Twitter</em>;\n",
    "- T_Time_Zone: atributo que representa a zona de horário do usuário no <em>Twitter</em>;\n",
    "- T_StatusText: atributo que representa um texto breve a respeito do estado do usuário no <em>Twitter</em>;\n",
    "- T_Language: atributo que representa a língua do usuário no <em>Twitter</em>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INÍCIO DOS EXPERIMENTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entrando no diretório do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# nome do arquivo dataset\n",
    "input_file = 'teste1.json'\n",
    "input_encoding = 'utf8'\n",
    "\n",
    "# path para o diretorio do arquivo\n",
    "root_path = 'datasets'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marcilene\\Desktop\\Mestrado - JM\\Dissertacao\\experimento_artigo_brasnam\n"
     ]
    }
   ],
   "source": [
    "# obtenção do diretório corrente\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# se o diretorio atual não conter \"datasets\" como as oito últimas letras do path, mude para o diretorio datasets\n",
    "if current_directory[-8:] != root_path:\n",
    "    os.chdir(root_path)\n",
    "\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset carregado com sucesso!\n",
      "número de registros:  10571\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(input_file, encoding=input_encoding) as json_file:\n",
    "    dataset = json_file.readlines()\n",
    "\n",
    "    for i, item in enumerate(dataset):\n",
    "        dataset[i] = json.loads(item)\n",
    "\n",
    "print(\"Dataset carregado com sucesso!\")\n",
    "print(\"número de registros: \", len(dataset))\n",
    "\n",
    "#print()\n",
    "#print(\"Analisando o layout das informações:\")\n",
    "#print()\n",
    "#print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Transformando o dataset json em DataFrame do Pandas\n",
    "\n",
    "Para maior facilidade no processo de análise exploratória de dados e também na manipulação das informações, o dataset será convertido para um DataFrame da biblioteca pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "#dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Comentários\n",
    "\n",
    "Pela análise da saída do comando \"dataset.describe()\", podemos obter algumas informações valiosas a respeito dos dados do dataset.\n",
    "\n",
    "De forma qualitativa, percebemos que no serviço <em>Twitter</em> os usuários tendem a preencher melhor os campos de perfil de usuário, enquanto no serviço <em>Google+</em> alguns desses mesmos usuários deixam informações em branco.\n",
    "\n",
    "Como exemplo, podemos citar que os campos G_FirstName e G_Lastname estão preenchidos em apenas 9410 registros, enquanto que os campos G_Displayname, T_ScreenName e T_Fullname estão preenchidos em todos os registros.\n",
    "\n",
    "Se levarmos em consideração que os campos relacionados ao nome do usuário são obrigatórios para este tipo de problema a ser tratado, os registros que estão com informação vazia devem ser limpos do dataset.\n",
    "\n",
    "Apesar da abordagem deste trabalho ser a identificação e associação de perfis de usuários a partir dos perfis disponíveis nos serviços, temos que os campos referentes ao nome são obrigatórios para podermos identificar os usuários.\n",
    "\n",
    "Dessa forma, nas próximas células, o registros com informação faltante serão removidos do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpeza do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Limpeza de registros contendo vazios vazios para atributos que representam nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpou os registros com dados faltantes\n",
      "número de registros após limpeza:  9410\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# se algum dos atributos do registro estiver vazio ou nulo,\n",
    "# o registro inteiro fica inválido não indo para a lista saneada.\n",
    "# ==========================================================================\n",
    "\n",
    "# remove todas as linhas que os atributos seguintes atributos sejam nulos:\n",
    "#     G_Firstname;\n",
    "#     G_Lastname;\n",
    "#     G_Displayname;\n",
    "#     T_Fullname;\n",
    "#     T_ScreenName;\n",
    "indexNames = dataset[ (dataset['G_Firstname'].isnull()) | (dataset['G_Lastname'].isnull()) | (dataset['G_Displayname'].isnull()) | (dataset['T_Fullname'].isnull()) | (dataset['T_ScreenName'].isnull()) ].index\n",
    "dataset.drop(indexNames , inplace=True)\n",
    "print(\"Limpou os registros com dados faltantes\")\n",
    "print(\"número de registros após limpeza: \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Comentários\n",
    "\n",
    "O número de registros após limpeza inicial corrobora com o número de registros com informações dos nomes preenchidas.\n",
    "\n",
    "Outra análise a ser feita é com relação aos dados disponíveis.\n",
    "\n",
    "Como era esperado, após analisar a saída do comando \"dataset.head()\" vemos que todas as colunas do dataset são categóricas, não sendo possível fazer nenhuma análise a partir do seu valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Separação dos registros em dois DataFrames separados\n",
    "\n",
    "Para facilitar a continuidade do processo de análise, vamos separar os registros entre dois conjuntos de dados, sendo um para o <em>Twitter</em> e outro para o <em>Google+</em>.\n",
    "\n",
    "<b>IMPORTANTE:</b>\n",
    "\n",
    "Serão considerados apenas o atributos que são iguais entre os perfis do <em>Google+</em> e <em>Twitter</em>, desta forma, apenas os seguinte atributos serão considerados:\n",
    "\n",
    "<em>Google+</em>:\n",
    "- G_Firstname;\n",
    "- G_Lastname;\n",
    "- G_Displayname;\n",
    "- G_aboutme;\n",
    "- G_Location;\n",
    "\n",
    "<em>Twitter</em>:\n",
    "- T_Fullname;\n",
    "- T_ScreenName;\n",
    "- T_Description;\n",
    "- T_Location;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separamos o dataset em Google+ e Twitter\n",
      "número de registros do Google+:  9410\n",
      "número de registros do Twitter:  9410\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# percorre o dataset e o separa em duas listas\n",
    "# uma lista para o google_plus e outra lista para o twitter\n",
    "# =========================================================================\n",
    "df_g = dataset[['Gid','G_Firstname', 'G_Location', 'G_Lastname', 'G_aboutme', 'G_Displayname']]\n",
    "df_t = dataset[['Tid','T_Fullname', 'T_Location', 'T_ScreenName', 'T_Description']]\n",
    "\n",
    "print(\"Separamos o dataset em Google+ e Twitter\")\n",
    "\n",
    "print(\"número de registros do Google+: \", len(df_g))\n",
    "print(\"número de registros do Twitter: \", len(df_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gid</th>\n",
       "      <th>G_Firstname</th>\n",
       "      <th>G_Location</th>\n",
       "      <th>G_Lastname</th>\n",
       "      <th>G_aboutme</th>\n",
       "      <th>G_Displayname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9410</td>\n",
       "      <td>9410</td>\n",
       "      <td>5178</td>\n",
       "      <td>9410</td>\n",
       "      <td>4382</td>\n",
       "      <td>9410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8367</td>\n",
       "      <td>5050</td>\n",
       "      <td>3211</td>\n",
       "      <td>6865</td>\n",
       "      <td>3815</td>\n",
       "      <td>8279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>114197951660575959146</td>\n",
       "      <td>David</td>\n",
       "      <td>London</td>\n",
       "      <td>.</td>\n",
       "      <td>PROVMEDIA - gratis online publiciteit voor ond...</td>\n",
       "      <td>������������</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>13</td>\n",
       "      <td>79</td>\n",
       "      <td>52</td>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Gid G_Firstname G_Location G_Lastname  \\\n",
       "count                    9410        9410       5178       9410   \n",
       "unique                   8367        5050       3211       6865   \n",
       "top     114197951660575959146       David     London          .   \n",
       "freq                       13          79         52         51   \n",
       "\n",
       "                                                G_aboutme G_Displayname  \n",
       "count                                                4382          9410  \n",
       "unique                                               3815          8279  \n",
       "top     PROVMEDIA - gratis online publiciteit voor ond...  ������������  \n",
       "freq                                                   13            18  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_g.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tid</th>\n",
       "      <th>T_Fullname</th>\n",
       "      <th>T_Location</th>\n",
       "      <th>T_ScreenName</th>\n",
       "      <th>T_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9410</td>\n",
       "      <td>9410</td>\n",
       "      <td>9410</td>\n",
       "      <td>9410</td>\n",
       "      <td>9410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8587</td>\n",
       "      <td>8543</td>\n",
       "      <td>4673</td>\n",
       "      <td>8587</td>\n",
       "      <td>6863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>783214</td>\n",
       "      <td>Twitter</td>\n",
       "      <td></td>\n",
       "      <td>twitter</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>1621</td>\n",
       "      <td>56</td>\n",
       "      <td>1843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Tid T_Fullname T_Location T_ScreenName T_Description\n",
       "count     9410       9410       9410         9410          9410\n",
       "unique    8587       8543       4673         8587          6863\n",
       "top     783214    Twitter                 twitter              \n",
       "freq        56         56       1621           56          1843"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Comentários\n",
    "\n",
    "Após executarmos as funções \"describe()\" nos dois novos DataFrames, fica mais fácil perceber que há diversos registros duplicados referentes a perfis do <em>Google+</em> e a mesma situação se repete nos perfis do <em>Twitter</em>, isto fica evidenciado pelo valor da linha \"unique\" nas colunas \"Gid\" e \"Tid\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Limpeza de registros duplicados\n",
    "\n",
    "A estratégia adotada é guardar o primeiro de cada grupo de repetidos, conforme célula de código abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# O dataset tem vários problemas de registros duplicados do google\n",
    "# e do twitter, mas nao estão repetidos juntos, assim, ao excluir os \n",
    "# repetidos do google e depois do twitter, acabamos com quantidades \n",
    "# diferentes em cada uma das listas de registros\n",
    "#\n",
    "# =========================================================================\n",
    "# ==========================================================================\n",
    "df_g.drop_duplicates(subset = 'Gid', keep = 'first', inplace = True)\n",
    "df_t.drop_duplicates(subset = 'Tid', keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gid</th>\n",
       "      <th>G_Firstname</th>\n",
       "      <th>G_Location</th>\n",
       "      <th>G_Lastname</th>\n",
       "      <th>G_aboutme</th>\n",
       "      <th>G_Displayname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8367</td>\n",
       "      <td>8367</td>\n",
       "      <td>4555</td>\n",
       "      <td>8367</td>\n",
       "      <td>3817</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8367</td>\n",
       "      <td>5049</td>\n",
       "      <td>3211</td>\n",
       "      <td>6864</td>\n",
       "      <td>3815</td>\n",
       "      <td>8278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>100890593900177935638</td>\n",
       "      <td>David</td>\n",
       "      <td>London</td>\n",
       "      <td>.</td>\n",
       "      <td>���������������������</td>\n",
       "      <td>������������</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>45</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Gid G_Firstname G_Location G_Lastname  \\\n",
       "count                    8367        8367       4555       8367   \n",
       "unique                   8367        5049       3211       6864   \n",
       "top     100890593900177935638       David     London          .   \n",
       "freq                        1          69         45         48   \n",
       "\n",
       "                    G_aboutme G_Displayname  \n",
       "count                    3817          8367  \n",
       "unique                   3815          8278  \n",
       "top     ���������������������  ������������  \n",
       "freq                        2            18  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_g.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tid</th>\n",
       "      <th>T_Fullname</th>\n",
       "      <th>T_Location</th>\n",
       "      <th>T_ScreenName</th>\n",
       "      <th>T_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8587</td>\n",
       "      <td>8587</td>\n",
       "      <td>8587</td>\n",
       "      <td>8587</td>\n",
       "      <td>8587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8587</td>\n",
       "      <td>8543</td>\n",
       "      <td>4673</td>\n",
       "      <td>8587</td>\n",
       "      <td>6863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>373273039</td>\n",
       "      <td>Kevin</td>\n",
       "      <td></td>\n",
       "      <td>hochul</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1483</td>\n",
       "      <td>1</td>\n",
       "      <td>1709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tid T_Fullname T_Location T_ScreenName T_Description\n",
       "count        8587       8587       8587         8587          8587\n",
       "unique       8587       8543       4673         8587          6863\n",
       "top     373273039      Kevin                  hochul              \n",
       "freq            1          4       1483            1          1709"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Comentários\n",
    "\n",
    "Como é possível notar, a remoção de registros com informações faltantes e a limpeza de registros duplicados fez com que acabássemos com duas listas de tamanho diferentes.\n",
    "\n",
    "Devemos atentar para o fato de que o número de registros diferentes não significa que tenhamos algum problema no decorrer do trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Carregamento dos conjuntos de treino e teste segundo autor do dataset\n",
    "\n",
    "Um ponto de atenção é que o problema de associação de perfis, difere da análise de registros individuais, sendo necessário avaliar sempre aos pares. Assim, cada avaliação é feita com base em dois perfis para analisar se são pertencentes à mesma pessoa ou não.\n",
    "\n",
    "Considerando que o dataset contém apenas registros corretamente pareados, ou seja, cada registro do dataset contém um perfil do <em>Google+</em> associado ao perfil do <em>Twitter</em>, em algum momento seria necessário criar um conjunto de treino composto de pares corretamente associados e pares associados de forma incorreta.\n",
    "\n",
    "Para prosseguirmos, será necessário criar os conjuntos de treino a fim de treinar o classificador. Contudo, o autor deste dataset, disponibilizou também 3 conjuntos para a etapa de treino e 2 conjuntos para a etapa de teste.\n",
    "\n",
    "Cabe ressaltar neste ponto, que cada conjunto de treino possui um \"nível de dificuldade\" diferente, sendo  o conjunto 1 o mais simples e o conjunto 3 o mais difícil.\n",
    "\n",
    "EXPLICAR OS NÍVEIS DE DIFICULDADE CONFORME AUTOR DO DATASET\n",
    "\n",
    "Nas células a seguir, vamos carregar estes conjuntos de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Carregamento dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregou dataset TrainGT1\n",
      "Conjunto de treino 1:  3500 registros.\n",
      "\n",
      "Carregou dataset TrainGT2\n",
      "Conjunto de treino 2:  3540 registros.\n",
      "\n",
      "Carregou dataset TrainGT3\n",
      "Conjunto de treino 3:  3550 registros.\n",
      "\n",
      "Carregou dataset TestGT1\n",
      "Conjunto de teste 1:  870 registros.\n",
      "\n",
      "Carregou dataset TestGT2\n",
      "Conjunto de teste 2:  663 registros.\n"
     ]
    }
   ],
   "source": [
    "dataset = None\n",
    "\n",
    "trainGT1 = pd.read_csv('TrainGT1.csv')\n",
    "print(\"Carregou dataset TrainGT1\")\n",
    "print(\"Conjunto de treino 1: \",len(trainGT1), \"registros.\")\n",
    "print()\n",
    "\n",
    "trainGT2 = pd.read_csv('TrainGT2.csv')\n",
    "print(\"Carregou dataset TrainGT2\")\n",
    "print(\"Conjunto de treino 2: \",len(trainGT2), \"registros.\")\n",
    "print()\n",
    "\n",
    "trainGT3 = pd.read_csv('TrainGT3.csv')\n",
    "print(\"Carregou dataset TrainGT3\")\n",
    "print(\"Conjunto de treino 3: \",len(trainGT3), \"registros.\")\n",
    "print()\n",
    "\n",
    "testGT1 = pd.read_csv('TestGT1.csv')\n",
    "print(\"Carregou dataset TestGT1\")\n",
    "print(\"Conjunto de teste 1: \",len(testGT1), \"registros.\")\n",
    "print()\n",
    "\n",
    "testGT2 = pd.read_csv('TestGT2.csv')\n",
    "print(\"Carregou dataset TestGT2\")\n",
    "print(\"Conjunto de teste 2: \",len(testGT2), \"registros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Comentátios\n",
    "\n",
    "Como pode ser visto em cada DataFrame dos conjuntos de treino e teste acima, eles contem apenas os identificadores do <em>Twitter</em> e <em>Google+</em> e o label indicando se pertencem à mesma pessoa ou não."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Montagem do conjunto de treino e teste contendo os atributos de perfil\n",
    "\n",
    "Para darmos continuidade é preciso montar uma estrutura de dados que tenha os atributos dos registros relacionados a cada identificador, além do label.\n",
    "\n",
    "Nas próximas células apresentamos a estrutura descrita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_gt(dataset_g, dataset_t, gabarito):\n",
    "    dataset_gt = []\n",
    "    labels_gt  = []\n",
    "\n",
    "    aux = {}\n",
    "\n",
    "    for index, row in gabarito.iterrows():\n",
    "\n",
    "        row_g = dataset_g.loc[dataset_g.Gid == str(row['Gid'])]\n",
    "        row_t = dataset_t.loc[dataset_t.Tid == str(row['Tid'])]\n",
    "\n",
    "        aux['g_plus']  = row_g\n",
    "        aux['twitter'] = row_t\n",
    "        dataset_gt.append(aux)\n",
    "\n",
    "        match = 1 if row['Same_identity'] == 'yes' else 0\n",
    "        labels_gt.append(match)\n",
    "        aux = {}\n",
    "\n",
    "    return dataset_gt, labels_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset de treino 1 obtido\n",
      "Conjunto de treino 1:  3500 registros.\n",
      "\n",
      "dataset de treino 2 obtido\n",
      "Conjunto de treino 2:  3540 registros.\n",
      "\n",
      "dataset de treino 3 obtido\n",
      "Conjunto de treino 3:  3550 registros.\n",
      "\n",
      "dataset de teste 1 obtido\n",
      "Conjunto de teste 1:  870 registros.\n",
      "\n",
      "dataset de teste 2 obtido\n",
      "Conjunto de teste 2:  663 registros.\n"
     ]
    }
   ],
   "source": [
    "dataset_treino_gt_1, labels_treino_gt_1 = make_train_gt(df_g, df_t, trainGT1)\n",
    "print(\"dataset de treino 1 obtido\")\n",
    "print(\"Conjunto de treino 1: \",len(dataset_treino_gt_1), \"registros.\")\n",
    "print()\n",
    "\n",
    "dataset_treino_gt_2, labels_treino_gt_2 = make_train_gt(df_g, df_t, trainGT2)\n",
    "print(\"dataset de treino 2 obtido\")\n",
    "print(\"Conjunto de treino 2: \",len(dataset_treino_gt_2), \"registros.\")\n",
    "print()\n",
    "\n",
    "dataset_treino_gt_3, labels_treino_gt_3 = make_train_gt(df_g, df_t, trainGT3)\n",
    "print(\"dataset de treino 3 obtido\")\n",
    "print(\"Conjunto de treino 3: \",len(dataset_treino_gt_3), \"registros.\")\n",
    "print()\n",
    "\n",
    "dataset_test_gt_1, labels_test_gt_1 = make_train_gt(df_g, df_t, testGT1)\n",
    "print(\"dataset de teste 1 obtido\")\n",
    "print(\"Conjunto de teste 1: \",len(dataset_test_gt_1), \"registros.\")\n",
    "print()\n",
    "\n",
    "dataset_test_gt_2, labels_test_gt_2 = make_train_gt(df_g, df_t, testGT2)\n",
    "print(\"dataset de teste 2 obtido\")\n",
    "print(\"Conjunto de teste 2: \",len(dataset_test_gt_2), \"registros.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Comentários\n",
    "\n",
    "O número de registros está igual ao número de registros dos conjuntos de de treino e teste propostos pelo autor do dataset, indicando que a função criada para montar a estrutura funcionou.\n",
    "\n",
    "A estrutura de dados é uma lista contendo dicionários como elementos. Cada dicionário contém duas chaves uma representando o registro do <em>Google+</em> e outra representando o <em>Twitter</em>. Os valores de cada uma dessas chaves são dataframes contendo os atributos.\n",
    "\n",
    "A partir deste momento, com registros contendo pares de perfis corretamente e incorretamente associados, é preciso pensar em modos de obter características que permitam aos algoritmos fazer a melhor classificação possível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Obtenção das Características\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando que os atributos são categóricos do tipo \"string\", e analisando a literatura disponível a respeito deste tipo de problema, pretendemos utilizar métricas de distância entre os valores das strings que correspondem a cada atributo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Funções que definem as métricas de similaridade de acordo com Esfandyari\n",
    "\n",
    "Nas células abaixo são definidas as funções que determinam a quanto duas strings são similares de acordo com as seguintes técnicas:\n",
    "\n",
    "- Exact Match (EM): comparação exata dos dois valores de entrada;\n",
    "- Longest Common Substring (LCS): a cadeia mais longa em comum. Em geral, este valor é normalizado, dividindo-se pela média do tamanhos das duas strings de entrada;\n",
    "- Longest Common Sub-Sequence (LCSS): Medida parecida com a LCS, porém de forma que a sequência não precise ser contígua. Novamente o valor de retorno é normalizado pela média do tamanho das duas strings orignais;\n",
    "- Levenshtein Distance (LD): o algoritmo de Levenshtein calcula o número mínimo de operações de edição que são necessárias para modificar uma string de forma à obter outra string;\n",
    "- Jaccard Similarity (JS): é o cálculo do tamanho da interseção de termos (por exemplo, palavras) dividida pelo tamanho da união dos conjuntos dos termos das entradas e;\n",
    "- Cosine Similarity (CS) with TF-IDF Weights: a semelhança de cosseno entre dois documentos mede o ângulo entre suas representações no modelo de espaço vetorial, que pode ser dada pela técnica tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUjGXER3g61l"
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def exactMatch(str1, str2):\n",
    "    str1 = str1.lower()\n",
    "    str2 = str2.lower()\n",
    "    if str1 == str2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def longestSubstringNormalized(str1,str2):\n",
    "    # initialize SequenceMatcher object with\n",
    "    # input string\n",
    "\n",
    "    str1 = str1.lower()\n",
    "    str2 = str2.lower()\n",
    "\n",
    "    seqMatch = SequenceMatcher(None,str1,str2)\n",
    "    # find match of longest sub-string\n",
    "    # output will be like Match(a=0, b=0, size=5)\n",
    "    match = seqMatch.find_longest_match(0, len(str1), 0, len(str2))\n",
    "    # print longest substring\n",
    "    if (match.size!=0):\n",
    "        a = len(str1) + len(str2)\n",
    "        b = a / 2\n",
    "        return (match.size / b)\n",
    "\n",
    "    return 0\n",
    "\n",
    "def lcssNormalized(str1, str2):\n",
    "    str1 = str1.lower()\n",
    "    str2 = str2.lower()\n",
    "    # find the length of the strings\n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    "\n",
    "    # declaring the array for storing the dp values\n",
    "    L = [[None]*(n + 1) for i in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0 :\n",
    "                L[i][j] = 0\n",
    "            elif str1[i-1] == str2[j-1]:\n",
    "                L[i][j] = L[i-1][j-1]+1\n",
    "            else:\n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    "    return (L[m][n] / ((m + n) / 2) )\n",
    "\n",
    "def edit_distance(str1, str2):\n",
    "    return nltk.edit_distance(str1.lower(), str2.lower())\n",
    "\n",
    "def jaccard_distance(str1, str2):\n",
    "    return nltk.jaccard_distance(set(str1.lower()), set(str2.lower()))\n",
    "\n",
    "def cosine_similarity_with_tf_idf(tfidf_vectorizer, str1, str2):\n",
    "    #[registro['G_aboutme'], registro['T_Description']])\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([str1.lower(), str2.lower()])\n",
    "    X = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "    return X[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ochZIAyTh8Tb"
   },
   "source": [
    "### 5.2 Características utilizadas\n",
    "\n",
    "As seguintes métricas serão empregadas para comparar o **campo nome completo** dos usuários:\n",
    "\n",
    "* Exact Match;\n",
    "* Longest Commom Substring;\n",
    "* Longest Commom SubSequence;\n",
    "* Levenshtein Distance e;\n",
    "* Jaccard Similarity.\n",
    "\n",
    "As seguintes métricas serão empregadas para comparar o **campo screen name** dos usuários:\n",
    "\n",
    "* Exact Match;\n",
    "* Longest Commom Substring;\n",
    "* Longest Commom SubSequence;\n",
    "* Levenshtein Distance e;\n",
    "* Jaccard Similarity.\n",
    "\n",
    "As seguintes métricas serão empregadas para comparar o **campo localização** dos usuários:\n",
    "\n",
    "* Exact Match;\n",
    "* Longest Commom Substring e;\n",
    "* Jaccard Similarity.\n",
    "\n",
    "As seguintes métricas serão empregadas para comparar o **campo descrição** dos usuários:\n",
    "\n",
    "* Cosine Similarity with TF-IDF Weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vMFY0PKlb97"
   },
   "outputs": [],
   "source": [
    "def get_metricas_fullname_italiana(str1, str2):\n",
    "    # 1 é bom, 0 é ruim\n",
    "    fullname_em   = exactMatch(str1, str2)\n",
    "\n",
    "    # quanto maior, melhor (de 0 até 1)\n",
    "    fullname_lcs  = longestSubstringNormalized(str1, str2)\n",
    "\n",
    "    # quanto maior, melhor (de 0 até 1)\n",
    "    fullname_lcss = lcssNormalized(str1, str2)\n",
    "\n",
    "    # quanto menor, melhor (valor absoluto)\n",
    "    fullname_ld   = edit_distance(str1, str2)\n",
    "\n",
    "    # quanto menor, melhor (de 0 até 1)\n",
    "    fullname_js   = jaccard_distance(str1, str2)\n",
    "\n",
    "    return [fullname_em, fullname_lcs, fullname_lcss, fullname_ld, fullname_js]\n",
    "\n",
    "def get_metricas_username_italiana(str1, str2):\n",
    "    # 1 é bom, 0 é ruim\n",
    "    username_em   = exactMatch(str1, str2)\n",
    "\n",
    "    # quanto maior, melhor (de 0 até 1)\n",
    "    username_lcs  = longestSubstringNormalized(str1, str2)\n",
    "\n",
    "    # quanto maior, melhor (de 0 até 1)\n",
    "    username_lcss = lcssNormalized(str1, str2)\n",
    "\n",
    "    # quanto menor, melhor (valor absoluto)\n",
    "    username_ld   = edit_distance(str1, str2)\n",
    "\n",
    "    # quanto menor, melhor (de 0 até 1)\n",
    "    username_js   = jaccard_distance(str1, str2)\n",
    "\n",
    "    return [username_em, username_lcs, username_lcss, username_ld, username_js]\n",
    "\n",
    "def get_metricas_location_italiana(str1, str2):\n",
    "     # 1 é bom, 0 é ruim\n",
    "    location_em  = exactMatch(str1, str2)\n",
    "\n",
    "    # quanto maior, melhor (de 0 até 1)\n",
    "    location_lcs = longestSubstringNormalized(str1, str2)\n",
    "\n",
    "    # quanto menor, melhor (de 0 até 1)\n",
    "    location_js  = jaccard_distance(str1, str2)\n",
    "\n",
    "    return [location_em, location_lcs, location_js]\n",
    "\n",
    "def get_metricas_description_italiana(tfidf_vectorizer, str1, str2):\n",
    "    # quanto maior, melhor (de 0 até 1)\n",
    "    if str1 == '' or str2 == '':\n",
    "        return [0.0]\n",
    "\n",
    "    description_cs = cosine_similarity_with_tf_idf(tfidf_vectorizer, str1, str2)\n",
    "\n",
    "    return [description_cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Obtenção do vetor de características\n",
    "\n",
    "Nas células a seguir são definidas as funções que montam o vetor de características a ser utilizado pelos classificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9eQEMyQmlqt"
   },
   "outputs": [],
   "source": [
    "def get_dict_metricas_italiana(vec_username, vec_fullname, vec_location, vec_description):\n",
    "    # ==========================================================================\n",
    "    # esta função recebe 4 vetores contendo:\n",
    "    # 1º vetor: par de screen names;\n",
    "    # 2º vetor: par de nomes completos;\n",
    "    # 3º vetor: par de localizaçãão;\n",
    "    # 4º vetor: par de descriçãão;\n",
    "\n",
    "    # Observação: na descrição, o elemento zero do vetor é o TfidfVectorizer,\n",
    "    #             e os elementos 1 e 2 correspondem ao par de descrição.\n",
    "    # ==========================================================================\n",
    "\n",
    "    vec_username_features    = get_metricas_username_italiana(vec_username[0], vec_username[1])\n",
    "    vec_fullname_features    = get_metricas_fullname_italiana(vec_fullname[0], vec_fullname[1])\n",
    "    vec_location_features    = get_metricas_location_italiana(vec_location[0], vec_location[1])\n",
    "    \n",
    "    vec_description_features = get_metricas_description_italiana(\n",
    "        vec_description[0], \n",
    "        vec_description[1], \n",
    "        vec_description[2]\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        vec_username_features[0],\n",
    "        vec_username_features[1],\n",
    "        vec_username_features[2],\n",
    "        vec_username_features[3],\n",
    "        vec_username_features[4],\n",
    "        vec_fullname_features[0],\n",
    "        vec_fullname_features[1],\n",
    "        vec_fullname_features[2],\n",
    "        vec_fullname_features[3],\n",
    "        vec_fullname_features[4],\n",
    "        vec_location_features[0],\n",
    "        vec_location_features[1],\n",
    "        vec_location_features[2],\n",
    "        vec_description_features[0]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LKPw_N2n56Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extrair_caracteristicas_gt(dataset):\n",
    "    # =============================================================================\n",
    "    # O for abaixo gera a lista de dicionarios contendo as caracteristicas\n",
    "    # de cada registro.\n",
    "    #\n",
    "    # no google, username é o G_Displayname;\n",
    "    # no twitter, username é o T_ScreenName;\n",
    "    #\n",
    "    # no google, fullname é o \"G_Firstname\"+\" \"+\"G_Lastname\";\n",
    "    # no twitter, fullname é o T_Fullname;\n",
    "    # =============================================================================\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    caracteristicas_italiana  = []\n",
    "    caracteristicas_chines    = []\n",
    "\n",
    "    for registro in dataset:\n",
    "\n",
    "        g_fullname = registro['g_plus']['G_Firstname'].values[0] + \" \" + registro['g_plus']['G_Lastname'].values[0]\n",
    "\n",
    "        vec_username    = [registro['g_plus']['G_Displayname'].values[0], registro['twitter']['T_ScreenName'].values[0]]\n",
    "        vec_fullname    = [g_fullname, registro['twitter']['T_Fullname'].values[0]]\n",
    "        vec_location    = [registro['g_plus']['G_Location'].values[0], registro['twitter']['T_Location'].values[0]]\n",
    "        vec_description = [tfidf_vectorizer, registro['g_plus']['G_aboutme'].values[0], registro['twitter']['T_Description'].values[0]]\n",
    "\n",
    "        aux = get_dict_metricas_italiana(\n",
    "            vec_username,\n",
    "            vec_fullname,\n",
    "            vec_location,\n",
    "            vec_description\n",
    "        )\n",
    "        caracteristicas_italiana.append(aux)\n",
    "\n",
    "    return caracteristicas_italiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características de treino 1 obtidas\n",
      "Características de treino 2 obtidas\n",
      "Características de treino 3 obtidas\n",
      "Características de teste 1 obtidas\n",
      "Características de teste 2 obtidas\n"
     ]
    }
   ],
   "source": [
    "dt_treinos = []\n",
    "dt_testes = []\n",
    "caracteristicas_treino_gt_1_italiana = extrair_caracteristicas_gt(dataset_treino_gt_1)\n",
    "dt_treinos.append((\"Treino 1_I\", caracteristicas_treino_gt_1_italiana, labels_treino_gt_1))\n",
    "print(\"Características de treino 1 obtidas\")\n",
    "\n",
    "caracteristicas_treino_gt_2_italiana = extrair_caracteristicas_gt(dataset_treino_gt_2)\n",
    "dt_treinos.append((\"Treino 2_I\", caracteristicas_treino_gt_2_italiana, labels_treino_gt_2))\n",
    "print(\"Características de treino 2 obtidas\")\n",
    "\n",
    "caracteristicas_treino_gt_3_italiana = extrair_caracteristicas_gt(dataset_treino_gt_3)\n",
    "dt_treinos.append((\"Treino 3_I\", caracteristicas_treino_gt_3_italiana, labels_treino_gt_3))\n",
    "print(\"Características de treino 3 obtidas\")\n",
    "\n",
    "caracteristicas_test_gt_1_italiana = extrair_caracteristicas_gt(dataset_test_gt_1)\n",
    "dt_testes.append((\"Teste 1_I\", caracteristicas_test_gt_1_italiana, labels_test_gt_1))\n",
    "print(\"Características de teste 1 obtidas\")\n",
    "\n",
    "caracteristicas_test_gt_2_italiana = extrair_caracteristicas_gt(dataset_test_gt_2)\n",
    "dt_testes.append((\"Teste 2_I\", caracteristicas_test_gt_2_italiana, labels_test_gt_2))\n",
    "print(\"Características de teste 2 obtidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Comentários\n",
    "\n",
    "Após a obtenção dos vetores de características de cada conjunto de treino e teste, eles passarão por algoritmos classificadores para obtenção de modelos que sejam capazes de detectar quando um perfil é realmente associado a outro de forma correta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlJjeNngaqMl"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def get_LogisticRegressionKF(kfold, scoring):\n",
    "    # Testa para descobrir o melhor conjunto de hiper-parâmetros para o classificador\n",
    "    # C's\n",
    "    C = [0.001, 0.01, 0.1, 1, 10]\n",
    "    # Gammas\n",
    "    solver = ['liblinear']\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {\n",
    "        'C' : C,\n",
    "        'solver' : solver\n",
    "    }\n",
    "    # Random search of parameters\n",
    "    random = GridSearchCV(\n",
    "        estimator = LogisticRegression(),\n",
    "        param_grid = random_grid,\n",
    "        cv = kfold,\n",
    "        scoring = scoring,\n",
    "        verbose = 0\n",
    "    )\n",
    "    return random\n",
    "\n",
    "def get_LogisticRegression(kfold, scoring):\n",
    "    return LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4DOr4B5AiBd"
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def get_LinearDiscriminantAnalysisKF(kfold, scoring):\n",
    "    # create random grid\n",
    "    random_grid = {}\n",
    "    # Random search of parameters\n",
    "    random = GridSearchCV(\n",
    "        estimator = LinearDiscriminantAnalysis(),\n",
    "        param_grid = random_grid,\n",
    "        cv = kfold,\n",
    "        scoring = scoring,\n",
    "        verbose = 0\n",
    "    )\n",
    "    return random\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def get_LinearDiscriminantAnalysis(kfold, scoring):\n",
    "    return LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gI9pCOvUCNkO"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def get_KNeighborsClassifierKF(kfold, scoring):\n",
    "    # Testa para descobrir o melhor conjunto de hiper-parâmetros para o classificador\n",
    "    # N Neighbors's\n",
    "    n_neighbors = range(2, 10)\n",
    "    # Weights\n",
    "    weights = ('uniform', 'distance', )\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {\n",
    "        'n_neighbors': n_neighbors,\n",
    "        'weights': weights\n",
    "    }\n",
    "    # Random search of parameters\n",
    "    random = GridSearchCV(\n",
    "        estimator = KNeighborsClassifier(),\n",
    "        param_grid = random_grid,\n",
    "        cv = kfold,\n",
    "        scoring = scoring,\n",
    "        verbose = 0\n",
    "    )\n",
    "    return random\n",
    "\n",
    "def get_KNeighborsClassifier(kfold, scoring):\n",
    "    return KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_pqlJObaxDP"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy\n",
    "\n",
    "def get_DecisionTreeClassifierKF(kfold, scoring):\n",
    "    # Testa para descobrir o melhor conjunto de hiper-parâmetros para o classificador\n",
    "    # profundidade máxima\n",
    "    max_depth = [int(x) for x in numpy.linspace(1, 500, num = 10)]\n",
    "    # número de características em cada split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    max_leaf_nodes = [int(x) for x in numpy.linspace(2, 500, num = 10)]\n",
    "    min_samples_leaf = [int(x) for x in numpy.linspace(2, 500, num = 10)]\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {\n",
    "        'min_samples_leaf' : min_samples_leaf,\n",
    "        'max_leaf_nodes'   : max_leaf_nodes,\n",
    "        'max_features'     : max_features,\n",
    "        'max_depth'        : max_depth\n",
    "    }\n",
    "    # Random search of parameters\n",
    "    random = GridSearchCV(\n",
    "        estimator = DecisionTreeClassifier(),\n",
    "        param_grid = random_grid,\n",
    "        cv = kfold,\n",
    "        scoring = scoring,\n",
    "        verbose = 0\n",
    "    )\n",
    "    return random\n",
    "\n",
    "def get_DecisionTreeClassifier(kfold, scoring):\n",
    "    return DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ya39TZ2aztZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def get_GaussianNBKF(kfold, scoring):\n",
    "    # Testa para descobrir o melhor conjunto de hiper-parâmetros para o classificador\n",
    "    # create random grid\n",
    "    random_grid = {}\n",
    "    # Random search of parameters\n",
    "    random = GridSearchCV(\n",
    "        estimator = GaussianNB(),\n",
    "        param_grid = random_grid,\n",
    "        cv = kfold,\n",
    "        scoring = scoring,\n",
    "        verbose = 0\n",
    "    )\n",
    "    return random\n",
    "\n",
    "def get_GaussianNB(kfold, scoring):\n",
    "    return GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6Hubkpha0fc"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def get_SVCKF(kfold, scoring):\n",
    "    # Testa para descobrir o melhor conjunto de hiper-parâmetros para o classificador\n",
    "    # C's\n",
    "    C = [0.001, 0.01, 0.1, 1, 10]\n",
    "    # Gammas\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    # Kernels\n",
    "    kernels = ['linear']\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {\n",
    "        'C' : C,\n",
    "        'gamma' : gammas,\n",
    "        'kernel' : kernels\n",
    "    }\n",
    "    # Random search of parameters\n",
    "    random = GridSearchCV(\n",
    "        estimator = SVC(),\n",
    "        param_grid = random_grid,\n",
    "        cv = kfold,\n",
    "        scoring = scoring,\n",
    "        verbose = 0\n",
    "    )\n",
    "    return random\n",
    "\n",
    "def get_SVC(kfold, scoring):\n",
    "    return SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZni0Wx4a4xM"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def get_RandomForestClassifierKF(kfold, scoring):\n",
    "    # Testa para descobrir o melhor conjunto de hiper-parâmetros para o classificador\n",
    "    # número de árvores na floresta aleatória\n",
    "    n_estimators = [int(x) for x in numpy.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    # número de características em cada split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "\n",
    "    # profundidade máxima\n",
    "    max_depth = [int(x) for x in numpy.linspace(100, 500, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # create random grid\n",
    "    random_grid = {\n",
    "        'n_estimators' : n_estimators,\n",
    "        'max_features' : max_features,\n",
    "        'max_depth'    : max_depth\n",
    "    }\n",
    "    # Random search of parameters\n",
    "    random = GridSearchCV(\n",
    "        estimator = RandomForestClassifier(),\n",
    "        param_grid = random_grid,\n",
    "        cv = kfold,\n",
    "        scoring = scoring,\n",
    "        verbose = 2,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    return random\n",
    "\n",
    "def get_RandomForestClassifier(kfold, scoring):\n",
    "    return RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gl20dkrADNoU"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def get_AdaBoostClassifierKF(kfold, scoring):\n",
    "    # Testa para descobrir o melhor conjunto de hiper-parâmetros para o classificador\n",
    "    # número de estimadores\n",
    "    n_estimators = [int(x) for x in numpy.linspace(start = 50, stop = 2000, num = 10)]\n",
    "    # taxa de aprendizado\n",
    "    learning_rate = [.1, 0.05, 0.01, 0.005, 0.001, ]\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "    # Random search of parameters\n",
    "    random = GridSearchCV(\n",
    "        estimator = AdaBoostClassifier(),\n",
    "        param_grid = random_grid,\n",
    "        cv = kfold,\n",
    "        scoring = scoring,\n",
    "        verbose = 0,\n",
    "    )\n",
    "    return random\n",
    "\n",
    "def get_AdaBoostClassifier(kfold, scoring):\n",
    "    return AdaBoostClassifier()\n",
    "\n",
    "import xgboost as xgb \n",
    "\n",
    "def get_XGBClassifierKF(kfold, scoring):\n",
    "\n",
    "    # Testa para descobrir o melhor conjunto de hiper-parâmetros para o classificador\n",
    "    # número de árvores na floresta aleatória\n",
    "    n_estimators = [int(x) for x in numpy.linspace(start = 200, stop = 1000, num = 10)]\n",
    "    # número de características em cada split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "\n",
    "    # profundidade máxima\n",
    "    max_depth = [int(x) for x in numpy.linspace(1, 10, num = 2)]\n",
    "    max_depth.append(None)\n",
    "    # create random grid\n",
    "    random_grid = {\n",
    "        'n_estimators' : n_estimators,\n",
    "        'max_features' : max_features,\n",
    "        'max_depth'    : max_depth,\n",
    "        'nthread'      : [8],\n",
    "        'objective':['binary:logistic'],\n",
    "        'learning_rate': [0.05, 0.1, 0.5, 1, 2, 3, 4, 5], #so called `eta` value\n",
    "    }\n",
    "    # Random search of parameters\n",
    "    random = GridSearchCV(\n",
    "        estimator = xgb.XGBClassifier(),\n",
    "        param_grid = random_grid,\n",
    "        cv = kfold,\n",
    "        scoring = scoring,\n",
    "        verbose = 2,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    return random\n",
    "    \n",
    "    params = {\n",
    "        \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "        \"gamma\": uniform(0, 0.5),\n",
    "        \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "        \"max_depth\": randint(2, 6), # default 3\n",
    "        \"n_estimators\": randint(100, 150), # default 100\n",
    "        \"subsample\": uniform(0.6, 0.4)\n",
    "    }\n",
    "\n",
    "def get_XGBClassifier():\n",
    "    return xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRFdOjjKFIeG"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def get_MLPClassifierKF(kfold, scoring):\n",
    "    # Testa para descobrir o melhor conjunto de hiper-parâmetros para o classificador\n",
    "    solver = ['sgd', 'adam']\n",
    "    learning_rate = ['constant', 'invscaling']\n",
    "    momentum = [0, .9]\n",
    "    nesterovs_momentum = [True, False]\n",
    "    learning_rate_init = [0.01, 0.2]\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {\n",
    "        'solver': solver,\n",
    "        'learning_rate': learning_rate,\n",
    "        'momentum': momentum,\n",
    "        'nesterovs_momentum': nesterovs_momentum,\n",
    "        'learning_rate_init': learning_rate_init\n",
    "    }\n",
    "    # Random search of parameters\n",
    "    random = GridSearchCV(\n",
    "        estimator = MLPClassifier(),\n",
    "        param_grid = random_grid,\n",
    "        cv = kfold,\n",
    "        scoring = scoring,\n",
    "        verbose = 0,\n",
    "    )\n",
    "    return random\n",
    "\n",
    "def get_MLPClassifier(kfold, scoring):\n",
    "    return MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execução do Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Marcilene\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#    SEM GRIDSEARCHCV\n",
    "#\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "scoring = 'accuracy'\n",
    "inner_kfold = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "\n",
    "#LISTA DE MODELOS\n",
    "models_sem_cv = []\n",
    "models_sem_cv.append(('LR', get_LogisticRegression(inner_kfold, scoring)))\n",
    "models_sem_cv.append(('LDA', get_LinearDiscriminantAnalysis(inner_kfold, scoring)))\n",
    "models_sem_cv.append(('K-NN', get_KNeighborsClassifier(inner_kfold, scoring)))\n",
    "models_sem_cv.append(('DT', get_DecisionTreeClassifier(inner_kfold, scoring)))\n",
    "models_sem_cv.append(('GNB', get_GaussianNB(inner_kfold, scoring)))\n",
    "models_sem_cv.append(('SVM', get_SVC(inner_kfold, scoring)))\n",
    "models_sem_cv.append(('RF', get_RandomForestClassifier(inner_kfold, scoring)))\n",
    "models_sem_cv.append(('AB', get_AdaBoostClassifier(inner_kfold, scoring)))\n",
    "models_sem_cv.append(('XGB', get_AdaBoostClassifier(inner_kfold, scoring)))\n",
    "models_sem_cv.append(('MLP', get_MLPClassifier(inner_kfold, scoring)))\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "fold_scores = []\n",
    "execucao = []\n",
    "\n",
    "#para cada dataset faço o split do kfold pra não roubar\n",
    "for dataset_name, features, labels in dt_treinos:\n",
    "    X_sparse = coo_matrix(features)\n",
    "    X_train, X_sparse, y_train = shuffle(features, X_sparse, labels)\n",
    "\n",
    "    X_train = numpy.array(X_train)\n",
    "    y_train = numpy.array(y_train)\n",
    "\n",
    "    #para cada modelo\n",
    "    for model_name, model in models_sem_cv:\n",
    "        \n",
    "        #inicio as variaveis\n",
    "        execution_name   = model_name + \" - \" + dataset_name\n",
    "        execution_scores = []\n",
    "        execution_stats  = {\"min\": 0, \"max\": 0, \"mean\": 0, \"median\": 0, \"std\": 0, \"fit-time\": 0}\n",
    "        \n",
    "        #instancio um modelo novo\n",
    "        if model_name == \"LR\":\n",
    "            model = LogisticRegression()\n",
    "        elif model_name == \"LDA\":\n",
    "            model = LinearDiscriminantAnalysis()\n",
    "        elif model_name == \"K-NN\":\n",
    "            model = KNeighborsClassifier()\n",
    "        elif model_name == \"DT\":\n",
    "            model = DecisionTreeClassifier()\n",
    "        elif model_name == \"GNB\":\n",
    "            model = GaussianNB()\n",
    "        elif model_name == \"SVM\":\n",
    "            model = SVC()\n",
    "        elif model_name == \"RF\":\n",
    "            model = RandomForestClassifier()\n",
    "        elif model_name == \"AB\":\n",
    "            model = AdaBoostClassifier()\n",
    "        elif model_name == \"XGB\":\n",
    "            model = xgb.XGBClassifier()\n",
    "        elif model_name == \"MLP\":\n",
    "            model = MLPClassifier()\n",
    "        \n",
    "        start = time.time()\n",
    "        #uso o split do kfold para ser igual entre os modelos\n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            model.fit(X_train[train_index], y_train[train_index])\n",
    "            execution_scores.append(model.score(X_train[test_index], y_train[test_index]))\n",
    "        \n",
    "        execution_stats[\"fit-time\"]   = time.time() - start\n",
    "        execution_stats[\"max\"]    = numpy.max(execution_scores)\n",
    "        execution_stats[\"min\"]    = numpy.min(execution_scores)\n",
    "        execution_stats[\"mean\"]   = numpy.mean(execution_scores)\n",
    "        execution_stats[\"median\"] = numpy.median(execution_scores)\n",
    "        execution_stats[\"std\"]    = numpy.std(execution_scores)\n",
    "        \n",
    "        execucao.append((execution_name, model, execution_stats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR - Treino 1_I\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "LDA - Treino 1_I\n",
      "{'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}\n",
      "\n",
      "K-NN - Treino 1_I\n",
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "\n",
      "DT - Treino 1_I\n",
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': None, 'splitter': 'best'}\n",
      "\n",
      "GNB - Treino 1_I\n",
      "{'priors': None, 'var_smoothing': 1e-09}\n",
      "\n",
      "SVM - Treino 1_I\n",
      "{'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\n",
      "RF - Treino 1_I\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "AB - Treino 1_I\n",
      "{'algorithm': 'SAMME.R', 'base_estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': None}\n",
      "\n",
      "XGB - Treino 1_I\n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 100, 'n_jobs': 1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': None, 'silent': None, 'subsample': 1, 'verbosity': 1}\n",
      "\n",
      "MLP - Treino 1_I\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\n",
      "LR - Treino 2_I\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "LDA - Treino 2_I\n",
      "{'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}\n",
      "\n",
      "K-NN - Treino 2_I\n",
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "\n",
      "DT - Treino 2_I\n",
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': None, 'splitter': 'best'}\n",
      "\n",
      "GNB - Treino 2_I\n",
      "{'priors': None, 'var_smoothing': 1e-09}\n",
      "\n",
      "SVM - Treino 2_I\n",
      "{'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\n",
      "RF - Treino 2_I\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "AB - Treino 2_I\n",
      "{'algorithm': 'SAMME.R', 'base_estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': None}\n",
      "\n",
      "XGB - Treino 2_I\n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 100, 'n_jobs': 1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': None, 'silent': None, 'subsample': 1, 'verbosity': 1}\n",
      "\n",
      "MLP - Treino 2_I\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\n",
      "LR - Treino 3_I\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "LDA - Treino 3_I\n",
      "{'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}\n",
      "\n",
      "K-NN - Treino 3_I\n",
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "\n",
      "DT - Treino 3_I\n",
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': None, 'splitter': 'best'}\n",
      "\n",
      "GNB - Treino 3_I\n",
      "{'priors': None, 'var_smoothing': 1e-09}\n",
      "\n",
      "SVM - Treino 3_I\n",
      "{'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\n",
      "RF - Treino 3_I\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "AB - Treino 3_I\n",
      "{'algorithm': 'SAMME.R', 'base_estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': None}\n",
      "\n",
      "XGB - Treino 3_I\n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 100, 'n_jobs': 1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': None, 'silent': None, 'subsample': 1, 'verbosity': 1}\n",
      "\n",
      "MLP - Treino 3_I\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model, scores in execucao:\n",
    "    print(name)\n",
    "    print(model.get_params())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR - Treino 1_I - Teste 1_I\t0.002000\t0.941379\t0.945335\t0.941379\t0.941249\n",
      "LR - Treino 1_I - Teste 2_I\t0.002000\t0.815988\t0.817026\t0.806722\t0.810073\n",
      "LDA - Treino 1_I - Teste 1_I\t0.002001\t0.897701\t0.914272\t0.897701\t0.896668\n",
      "LDA - Treino 1_I - Teste 2_I\t0.001032\t0.838612\t0.845705\t0.849681\t0.838493\n",
      "K-NN - Treino 1_I - Teste 1_I\t0.035019\t0.896552\t0.909037\t0.896552\t0.895756\n",
      "K-NN - Treino 1_I - Teste 2_I\t0.026953\t0.782805\t0.779559\t0.783028\t0.780564\n",
      "DT - Treino 1_I - Teste 1_I\t0.002001\t0.927586\t0.927607\t0.927586\t0.927585\n",
      "DT - Treino 1_I - Teste 2_I\t0.001000\t0.815988\t0.828365\t0.800278\t0.806200\n",
      "GNB - Treino 1_I - Teste 1_I\t0.001991\t0.936782\t0.943363\t0.936782\t0.936546\n",
      "GNB - Treino 1_I - Teste 2_I\t0.002000\t0.812971\t0.811221\t0.806472\t0.808346\n",
      "SVM - Treino 1_I - Teste 1_I\t0.022995\t0.889655\t0.907900\t0.889655\t0.888407\n",
      "SVM - Treino 1_I - Teste 2_I\t0.021005\t0.761689\t0.759448\t0.763556\t0.759915\n",
      "RF - Treino 1_I - Teste 1_I\t0.019013\t0.955172\t0.955869\t0.955172\t0.955155\n",
      "RF - Treino 1_I - Teste 2_I\t0.016003\t0.838612\t0.848205\t0.825514\t0.831304\n",
      "AB - Treino 1_I - Teste 1_I\t0.012996\t0.955172\t0.956236\t0.955172\t0.955146\n",
      "AB - Treino 1_I - Teste 2_I\t0.012004\t0.843137\t0.850219\t0.831528\t0.836764\n",
      "XGB - Treino 1_I - Teste 1_I\t0.007010\t0.964368\t0.965453\t0.964368\t0.964347\n",
      "XGB - Treino 1_I - Teste 2_I\t0.006001\t0.846154\t0.855966\t0.833389\t0.839292\n",
      "MLP - Treino 1_I - Teste 1_I\t0.003000\t0.954023\t0.955651\t0.954023\t0.953982\n",
      "MLP - Treino 1_I - Teste 2_I\t0.003000\t0.831071\t0.837994\t0.818847\t0.823988\n",
      "LR - Treino 1_C - Teste 1_C\t0.003002\t0.936782\t0.939965\t0.936782\t0.936667\n",
      "LR - Treino 1_C - Teste 2_C\t0.003000\t0.822021\t0.822438\t0.813667\t0.816696\n",
      "LDA - Treino 1_C - Teste 1_C\t0.003000\t0.901149\t0.917466\t0.901149\t0.900174\n",
      "LDA - Treino 1_C - Teste 2_C\t0.002001\t0.838612\t0.844316\t0.848875\t0.838434\n",
      "K-NN - Treino 1_C - Teste 1_C\t0.071016\t0.893103\t0.904495\t0.893103\t0.892346\n",
      "K-NN - Treino 1_C - Teste 2_C\t0.040010\t0.819005\t0.819517\t0.825097\t0.818310\n",
      "DT - Treino 1_C - Teste 1_C\t0.003000\t0.926437\t0.926879\t0.926437\t0.926418\n",
      "DT - Treino 1_C - Teste 2_C\t0.002000\t0.829563\t0.843988\t0.813889\t0.820368\n",
      "GNB - Treino 1_C - Teste 1_C\t0.003000\t0.933333\t0.939135\t0.933333\t0.933112\n",
      "GNB - Treino 1_C - Teste 2_C\t0.003000\t0.803922\t0.802778\t0.796056\t0.798475\n",
      "SVM - Treino 1_C - Teste 1_C\t0.026006\t0.895402\t0.912677\t0.895402\t0.894296\n",
      "SVM - Treino 1_C - Teste 2_C\t0.019996\t0.834087\t0.841515\t0.845278\t0.833978\n",
      "RF - Treino 1_C - Teste 1_C\t0.020014\t0.963218\t0.964200\t0.963218\t0.963199\n",
      "RF - Treino 1_C - Teste 2_C\t0.016994\t0.843137\t0.853580\t0.829917\t0.835925\n",
      "AB - Treino 1_C - Teste 1_C\t0.015013\t0.959770\t0.960317\t0.959770\t0.959758\n",
      "AB - Treino 1_C - Teste 2_C\t0.013011\t0.850679\t0.860435\t0.838194\t0.844120\n",
      "XGB - Treino 1_C - Teste 1_C\t0.012994\t0.962069\t0.962776\t0.962069\t0.962054\n",
      "XGB - Treino 1_C - Teste 2_C\t0.009994\t0.847662\t0.858056\t0.834722\t0.840763\n",
      "MLP - Treino 1_C - Teste 1_C\t0.004991\t0.958621\t0.959797\t0.958621\t0.958594\n",
      "MLP - Treino 1_C - Teste 2_C\t0.004001\t0.841629\t0.849787\t0.829389\t0.834884\n",
      "LR - Treino 2_I - Teste 1_I\t0.002000\t0.921839\t0.931172\t0.921839\t0.921414\n",
      "LR - Treino 2_I - Teste 2_I\t0.001000\t0.882353\t0.884288\t0.890764\t0.882024\n",
      "LDA - Treino 2_I - Teste 1_I\t0.002001\t0.898851\t0.915870\t0.898851\t0.897805\n",
      "LDA - Treino 2_I - Teste 2_I\t0.001001\t0.856712\t0.868854\t0.870514\t0.856700\n",
      "K-NN - Treino 2_I - Teste 1_I\t0.034008\t0.852874\t0.874266\t0.852874\t0.850741\n",
      "K-NN - Treino 2_I - Teste 2_I\t0.030007\t0.808446\t0.828643\t0.825028\t0.808384\n",
      "DT - Treino 2_I - Teste 1_I\t0.002000\t0.889655\t0.889861\t0.889655\t0.889641\n",
      "DT - Treino 2_I - Teste 2_I\t0.002000\t0.882353\t0.880116\t0.885931\t0.881414\n",
      "GNB - Treino 2_I - Teste 1_I\t0.003000\t0.870115\t0.896898\t0.870115\t0.867886\n",
      "GNB - Treino 2_I - Teste 2_I\t0.002000\t0.825038\t0.851095\t0.843722\t0.824862\n",
      "SVM - Treino 2_I - Teste 1_I\t0.028006\t0.837931\t0.875141\t0.837931\t0.833810\n",
      "SVM - Treino 2_I - Teste 2_I\t0.029006\t0.788839\t0.829865\t0.811722\t0.787861\n",
      "RF - Treino 2_I - Teste 1_I\t0.020005\t0.942529\t0.945249\t0.942529\t0.942441\n",
      "RF - Treino 2_I - Teste 2_I\t0.017994\t0.918552\t0.916571\t0.923167\t0.917946\n",
      "AB - Treino 2_I - Teste 1_I\t0.012994\t0.926437\t0.931259\t0.926437\t0.926231\n",
      "AB - Treino 2_I - Teste 2_I\t0.011012\t0.898944\t0.898846\t0.905833\t0.898497\n",
      "XGB - Treino 2_I - Teste 1_I\t0.007001\t0.936782\t0.940697\t0.936782\t0.936641\n",
      "XGB - Treino 2_I - Teste 2_I\t0.005001\t0.911011\t0.909882\t0.916903\t0.910502\n",
      "MLP - Treino 2_I - Teste 1_I\t0.002991\t0.929885\t0.935876\t0.929885\t0.929643\n",
      "MLP - Treino 2_I - Teste 2_I\t0.003001\t0.901961\t0.902698\t0.909708\t0.901603\n",
      "LR - Treino 2_C - Teste 1_C\t0.003001\t0.926437\t0.932146\t0.926437\t0.926193\n",
      "LR - Treino 2_C - Teste 2_C\t0.002002\t0.907994\t0.910211\t0.917056\t0.907751\n",
      "LDA - Treino 2_C - Teste 1_C\t0.003000\t0.895402\t0.913498\t0.895402\t0.894245\n",
      "LDA - Treino 2_C - Teste 2_C\t0.003002\t0.859729\t0.875814\t0.875194\t0.859727\n",
      "K-NN - Treino 2_C - Teste 1_C\t0.054009\t0.858621\t0.874288\t0.858621\t0.857126\n",
      "K-NN - Treino 2_C - Teste 2_C\t0.040009\t0.832579\t0.853472\t0.849583\t0.832524\n",
      "DT - Treino 2_C - Teste 1_C\t0.003001\t0.895402\t0.895404\t0.895402\t0.895402\n",
      "DT - Treino 2_C - Teste 2_C\t0.002001\t0.877828\t0.875062\t0.877500\t0.876113\n",
      "GNB - Treino 2_C - Teste 1_C\t0.003000\t0.902299\t0.917498\t0.902299\t0.901401\n",
      "GNB - Treino 2_C - Teste 2_C\t0.003001\t0.850679\t0.857843\t0.861958\t0.850569\n",
      "SVM - Treino 2_C - Teste 1_C\t0.033009\t0.873563\t0.899083\t0.873563\t0.871509\n",
      "SVM - Treino 2_C - Teste 2_C\t0.025009\t0.829563\t0.855162\t0.848125\t0.829407\n",
      "RF - Treino 2_C - Teste 1_C\t0.020017\t0.947126\t0.950565\t0.947126\t0.947025\n",
      "RF - Treino 2_C - Teste 2_C\t0.017002\t0.920060\t0.918181\t0.924903\t0.919486\n",
      "AB - Treino 2_C - Teste 1_C\t0.014013\t0.936782\t0.939965\t0.936782\t0.936667\n",
      "AB - Treino 2_C - Teste 2_C\t0.012988\t0.911011\t0.909343\t0.916097\t0.910416\n",
      "XGB - Treino 2_C - Teste 1_C\t0.013002\t0.941379\t0.945335\t0.941379\t0.941249\n",
      "XGB - Treino 2_C - Teste 2_C\t0.011002\t0.917044\t0.915917\t0.923042\t0.916569\n",
      "MLP - Treino 2_C - Teste 1_C\t0.004732\t0.939080\t0.943016\t0.939080\t0.938945\n",
      "MLP - Treino 2_C - Teste 2_C\t0.006002\t0.915535\t0.914561\t0.921708\t0.915072\n",
      "LR - Treino 3_I - Teste 1_I\t0.002001\t0.914943\t0.925973\t0.914943\t0.914388\n",
      "LR - Treino 3_I - Teste 2_I\t0.001000\t0.877828\t0.881730\t0.887569\t0.877610\n",
      "LDA - Treino 3_I - Teste 1_I\t0.002001\t0.889655\t0.909605\t0.889655\t0.888295\n",
      "LDA - Treino 3_I - Teste 2_I\t0.001000\t0.846154\t0.861475\t0.861181\t0.846153\n",
      "K-NN - Treino 3_I - Teste 1_I\t0.034008\t0.764368\t0.765109\t0.764368\t0.764203\n",
      "K-NN - Treino 3_I - Teste 2_I\t0.026006\t0.710407\t0.708080\t0.711375\t0.708251\n",
      "DT - Treino 3_I - Teste 1_I\t0.002001\t0.826437\t0.829658\t0.826437\t0.826012\n",
      "DT - Treino 3_I - Teste 2_I\t0.001000\t0.843137\t0.840149\t0.841194\t0.840640\n",
      "GNB - Treino 3_I - Teste 1_I\t0.002001\t0.859770\t0.890485\t0.859770\t0.856957\n",
      "GNB - Treino 3_I - Teste 2_I\t0.001000\t0.814480\t0.846038\t0.834792\t0.814099\n",
      "SVM - Treino 3_I - Teste 1_I\t0.028006\t0.826437\t0.869827\t0.826437\t0.821192\n",
      "SVM - Treino 3_I - Teste 2_I\t0.021005\t0.773756\t0.823368\t0.798792\t0.772071\n",
      "RF - Treino 3_I - Teste 1_I\t0.019014\t0.917241\t0.917674\t0.917241\t0.917220\n",
      "RF - Treino 3_I - Teste 2_I\t0.017994\t0.917044\t0.915160\t0.921833\t0.916448\n",
      "AB - Treino 3_I - Teste 1_I\t0.012002\t0.926437\t0.928757\t0.926437\t0.926337\n",
      "AB - Treino 3_I - Teste 2_I\t0.011994\t0.909502\t0.908227\t0.915167\t0.908964\n",
      "XGB - Treino 3_I - Teste 1_I\t0.008002\t0.927586\t0.930061\t0.927586\t0.927482\n",
      "XGB - Treino 3_I - Teste 2_I\t0.006001\t0.909502\t0.908227\t0.915167\t0.908964\n",
      "MLP - Treino 3_I - Teste 1_I\t0.002990\t0.896552\t0.896552\t0.896552\t0.896552\n",
      "MLP - Treino 3_I - Teste 2_I\t0.003001\t0.907994\t0.905901\t0.912222\t0.907284\n",
      "LR - Treino 3_C - Teste 1_C\t0.003001\t0.928736\t0.932033\t0.928736\t0.928599\n",
      "LR - Treino 3_C - Teste 2_C\t0.002001\t0.918552\t0.919071\t0.926389\t0.918239\n",
      "LDA - Treino 3_C - Teste 1_C\t0.003001\t0.905747\t0.917839\t0.905747\t0.905060\n",
      "LDA - Treino 3_C - Teste 2_C\t0.003001\t0.874811\t0.883689\t0.887319\t0.874756\n",
      "K-NN - Treino 3_C - Teste 1_C\t0.052012\t0.693103\t0.701540\t0.693103\t0.689858\n",
      "K-NN - Treino 3_C - Teste 2_C\t0.039009\t0.722474\t0.717531\t0.716000\t0.716656\n",
      "DT - Treino 3_C - Teste 1_C\t0.003000\t0.719540\t0.754892\t0.719540\t0.709467\n",
      "DT - Treino 3_C - Teste 2_C\t0.002000\t0.825038\t0.827604\t0.815125\t0.819009\n",
      "GNB - Treino 3_C - Teste 1_C\t0.003000\t0.901149\t0.916687\t0.901149\t0.900219\n",
      "GNB - Treino 3_C - Teste 2_C\t0.003000\t0.849170\t0.856695\t0.860625\t0.849071\n",
      "SVM - Treino 3_C - Teste 1_C\t0.033016\t0.868966\t0.896175\t0.868966\t0.866676\n",
      "SVM - Treino 3_C - Teste 2_C\t0.025998\t0.825038\t0.853698\t0.844528\t0.824789\n",
      "RF - Treino 3_C - Teste 1_C\t0.019994\t0.737931\t0.775157\t0.737931\t0.728757\n",
      "RF - Treino 3_C - Teste 2_C\t0.017004\t0.871795\t0.872218\t0.866125\t0.868561\n",
      "AB - Treino 3_C - Teste 1_C\t0.017007\t0.888506\t0.888754\t0.888506\t0.888488\n",
      "AB - Treino 3_C - Teste 2_C\t0.014003\t0.895928\t0.893195\t0.897528\t0.894754\n",
      "XGB - Treino 3_C - Teste 1_C\t0.013003\t0.852874\t0.858397\t0.852874\t0.852305\n",
      "XGB - Treino 3_C - Teste 2_C\t0.010002\t0.894419\t0.892169\t0.893375\t0.892738\n",
      "MLP - Treino 3_C - Teste 1_C\t0.004987\t0.768966\t0.797632\t0.768966\t0.763265\n",
      "MLP - Treino 3_C - Teste 2_C\t0.004000\t0.826546\t0.834264\t0.813639\t0.818927\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "dicts = []\n",
    "for name, model, scores in execucao:\n",
    "    for teste, features, labels in dt_testes:\n",
    "        if name[-1] == teste[-1]:\n",
    "            start = time.time()\n",
    "            predicted = model.predict(features)\n",
    "            fit_time = time.time() - start\n",
    "            report = classification_report(labels, predicted, output_dict = True)\n",
    "            df = pandas.DataFrame(report).transpose()\n",
    "            print(\"{}\\t{:08.6f}\\t{:08.6f}\\t{:08.6f}\\t{:08.6f}\\t{:08.6f}\".format(name + \" - \" + teste, fit_time, df.iloc[2,0], df.iloc[3,0], df.iloc[3,1], df.iloc[3,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CIDA_Trab_Final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
